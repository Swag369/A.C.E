{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarCoder2-7B LoRA Finetuning with Alpaca-Style Python Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers peft bitsandbytes datasets accelerate trl torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Open-Source Python Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "datasets_to_load = [\n",
    "    (\"codeparrot/github-code-clean\", \"Python\"),\n",
    "    (\"heegyu/MATH23K\", None),\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    print(\"\\n1. Loading GitHub Code (Python)...\")\n",
    "    github_ds = load_dataset(\"codeparrot/github-code-clean\", \"Python\", split=\"train\", streaming=True)\n",
    "    github_data = []\n",
    "    for i, sample in enumerate(github_ds):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        if sample.get('code'):\n",
    "            all_data.append({\n",
    "                \"instruction\": \"Complete the following Python code:\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": sample['code'][:2000]\n",
    "            })\n",
    "    print(f\"   Loaded {len(github_data)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n2. Loading Stack Overflow Q&A Dataset...\")\n",
    "    stackoverflow_ds = load_dataset(\"HuggingFaceH4/stack-exchange-qa\", split=\"train[:5000]\")\n",
    "    for sample in stackoverflow_ds:\n",
    "        if \"python\" in sample.get('tags', '').lower():\n",
    "            all_data.append({\n",
    "                \"instruction\": sample['title'][:200],\n",
    "                \"input\": sample['question'][:1000],\n",
    "                \"output\": sample['answer'][:2000]\n",
    "            })\n",
    "    print(f\"   Loaded {len(stackoverflow_ds)} Q&A samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n3. Loading CodeSearchNet (Python)...\")\n",
    "    codesearchnet_ds = load_dataset(\"code_search_net\", \"python\", split=\"train[:3000]\", trust_remote_code=True)\n",
    "    for sample in codesearchnet_ds:\n",
    "        if sample.get('code'):\n",
    "            all_data.append({\n",
    "                \"instruction\": f\"Implement: {sample.get('func_name', 'function')}\",\n",
    "                \"input\": sample.get('docstring', '')[:500],\n",
    "                \"output\": sample['code'][:2000]\n",
    "            })\n",
    "    print(f\"   Loaded CodeSearchNet samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n4. Loading The Stack Dataset (Python)...\")\n",
    "    stack_ds = load_dataset(\"bigcode/the-stack\", \"data\", split=\"train[:2000]\", streaming=True, trust_remote_code=True)\n",
    "    count = 0\n",
    "    for sample in stack_ds:\n",
    "        if sample.get('ext') == '.py' and sample.get('content'):\n",
    "            all_data.append({\n",
    "                \"instruction\": \"Complete the following Python code:\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": sample['content'][:2000]\n",
    "            })\n",
    "            count += 1\n",
    "            if count >= 1000:\n",
    "                break\n",
    "    print(f\"   Loaded {count} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n5. Loading HumanEval-Instruct...\")\n",
    "    humaneval_ds = load_dataset(\"TIGER-Lab/HumanEval-Instruct\", split=\"train\")\n",
    "    for sample in humaneval_ds:\n",
    "        all_data.append({\n",
    "            \"instruction\": sample.get('prompt', '')[:200],\n",
    "            \"input\": \"\",\n",
    "            \"output\": sample.get('canonical_solution', '')[:2000]\n",
    "        })\n",
    "    print(f\"   Loaded {len(humaneval_ds)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df = df.drop_duplicates(subset=['output'])\n",
    "print(f\"\\n\\nTotal unique samples: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Format Data in Alpaca Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alpaca_prompt(instruction: str, input_text: str, output: str) -> str:\n",
    "    if input_text:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "df['text'] = df.apply(\n",
    "    lambda row: create_alpaca_prompt(row['instruction'], row['input'], row['output']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Sample formatted prompt:\")\n",
    "print(df['text'].iloc[0])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df[['text']])\n",
    "\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train samples: {len(split_dataset['train'])}\")\n",
    "print(f\"Test samples: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bigcode/starcoder2-7b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"LoRA configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./starcoder2-7b-lora-python\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    eval_steps=25,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final loss: {train_result.training_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./starcoder2-7b-lora-python-final\")\n",
    "print(\"Model saved to ./starcoder2-7b-lora-python-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainer.push_to_hub(\"hemanthnov2001/starcoder2-7b-lora-python\")\n",
    "# print(\"Model pushed to HuggingFace Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./starcoder2-7b-lora-python-final\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "def test_model(instruction: str, input_text: str = \"\") -> str:\n",
    "    if input_text:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.2, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "test_prompt = \"Write a function to find the factorial of a number\"\n",
    "print(f\"Instruction: {test_prompt}\")\n",
    "print(f\"\\nResponse:\\n{test_model(test_prompt)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
