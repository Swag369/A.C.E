{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers accelerate bitsandbytes langchain langgraph\n",
        "# !pip install -q datasets"
      ],
      "metadata": {
        "id": "yi-kQ-oBxZSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Make sure we have a GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"GPU not found. Go to Runtime > Change runtime type > GPU\")\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "tPlJJ3xNx1hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(userdata.get('HUGGINGFACE_HUB_TOKEN'))\n"
      ],
      "metadata": {
        "id": "I9s9Dten1nql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"bigcode/starcoderbase-3b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,           # enables 4-bit quantization\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",   # NormalFloat4\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded in 4-bit successfully!\")\n"
      ],
      "metadata": {
        "id": "i3rsQDrPx5E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"def fibonacci(n):\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "C-HwSkKbx9-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU langchain-community langchain-core langchain-huggingface"
      ],
      "metadata": {
        "id": "6e6zXNeMADDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community langchain-core langchain-huggingface"
      ],
      "metadata": {
        "id": "echNiBTfAq_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TESTING REPL #####\n",
        "\n",
        "print(\"\\n--- Starting the Simple REPL Test ---\")\n",
        "\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "print(\"Creating PythonREPL instance...\")\n",
        "python_repl = PythonREPL()\n",
        "\n",
        "code_to_run = \"print(1+1)\"\n",
        "print(f\"Executing code: '{code_to_run}'\")\n",
        "\n",
        "result = python_repl.run(code_to_run)\n",
        "\n",
        "print(\"\\n--- Output from PythonREPL ---\")\n",
        "print(result)\n",
        "print(\"------------------------------\")"
      ],
      "metadata": {
        "id": "dLhXOTz7EeVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU langchain\n",
        "\n",
        "from langchain.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "python_repl_utility = PythonREPL()\n",
        "\n",
        "repl_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python REPL. Use this to execute python code. The input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl_utility.run\n",
        ")\n",
        "\n",
        "print(\"✅ Successfully wrapped the Python REPL in a LangChain Tool.\")\n",
        "print(f\"\\nTool Name: {repl_tool.name}\")\n",
        "print(f\"Tool Description: {repl_tool.description}\")\n",
        "\n",
        "print(\"\\n--- Testing the new Tool object ---\")\n",
        "test_result = repl_tool.run(\"print(5 * 5)\")\n",
        "print(f\"Result of 'repl_tool.run(\\\"print(5 * 5)\\\")':\\n{test_result}\")\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "id": "Pg8nYi5tEulP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"--- Preparing a prompt to test StarCoder's tool-using ability ---\")\n",
        "\n",
        "question = \"Calculate 25 raised to the power of 0.5 and tell me the result.\"\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are a helpful assistant that can use tools. You have access to the following tool:\n",
        "\n",
        "Tool Name: {repl_tool.name}\n",
        "Tool Description: {repl_tool.description}\n",
        "\n",
        "When you need to use a tool, you MUST respond in the following format:\n",
        "\n",
        "Action: [the name of the tool, e.g., python_repl]\n",
        "Action Input: [the input to the tool, e.g., print(2 + 2)]\n",
        "\n",
        "Here is your task:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- This is the full prompt the model will see ---\")\n",
        "print(prompt_template)\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_p=0.95,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the output and print it.\n",
        "print(\"\\n--- Raw output generated by StarCoder ---\")\n",
        "\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response_text)\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Verification ---\")\n",
        "if \"Action: python_repl\" in response_text and \"25 ** 0.5\" in response_text:\n",
        "    print(\"✅ SUCCESS! The model generated the correct Action and Action Input.\")\n",
        "    print(\"This confirms that we can successfully prompt StarCoder to use our tool.\")\n",
        "else:\n",
        "    print(\"❌ FAILED. The model did not generate the expected output format.\")\n",
        "    print(\"You might need to adjust the prompt or the generation parameters (like temperature).\")"
      ],
      "metadata": {
        "id": "LoHEM6C3E2zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU datasets langchain langchain-experimental\n",
        "\n",
        "from datasets import load_dataset\n",
        "from langchain.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "import re\n",
        "\n",
        "python_repl_utility = PythonREPL()\n",
        "repl_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python REPL. Use this to execute python code. The input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl_utility.run\n",
        ")\n",
        "\n",
        "print(\"✅ Setup complete. REPL tool and datasets library are ready.\")\n",
        "\n",
        "print(\"\\n--- Loading a problem from the HumanEval benchmark ---\")\n",
        "\n",
        "# Load the entire dataset\n",
        "humaneval_dataset = load_dataset(\"openai_humaneval\")\n",
        "\n",
        "# Problem: \"has_close_elements\" - Check if any two elements in a list are closer than a threshold.\n",
        "problem_index = 0\n",
        "problem = humaneval_dataset[\"test\"][problem_index]\n",
        "\n",
        "# The 'prompt' contains the function signature and docstring that the model should complete.\n",
        "humaneval_prompt = problem[\"prompt\"]\n",
        "# The 'test' contains the Python unit test code that verifies the solution.\n",
        "humaneval_test = problem[\"test\"]\n",
        "\n",
        "print(f\"Loaded HumanEval Problem #{problem_index}: {problem['entry_point']}\")\n",
        "print(\"\\n--- Function Stub and Docstring (Given to Model) ---\")\n",
        "print(humaneval_prompt)\n",
        "print(\"\\n--- Official Unit Test (Used for Verification) ---\")\n",
        "print(humaneval_test)\n",
        "\n",
        "\n",
        "print(\"\\n--- Prompting the base StarCoder model to solve the problem ---\")\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are an expert Python programmer. Your task is to complete the following function based on the docstring.\n",
        "You must respond with a plan and the code to execute in the python_repl tool.\n",
        "\n",
        "Use the following format:\n",
        "Action: python_repl\n",
        "Action Input: [your completed python code]\n",
        "\n",
        "Here is the function to complete:\n",
        "{humaneval_prompt}\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the prompt for the model\n",
        "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate the model's response\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256, # Give it enough room for a full function\n",
        "    temperature=0.2,    # Low temperature for more deterministic, less \"creative\" code\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode and print the model's raw response\n",
        "model_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Model's Raw Generated Response ---\")\n",
        "print(model_response)\n",
        "\n",
        "print(\"\\n--- Evaluating the model's solution ---\")\n",
        "\n",
        "# Use regex to find and extract only the Python code from the model's response.\n",
        "# We look for the code inside the \"Action Input\" block.\n",
        "code_match = re.search(r\"Action Input:\\s*```python\\n(.*?)\\n```\", model_response, re.DOTALL)\n",
        "if not code_match:\n",
        "    # A more lenient search if the above fails\n",
        "    code_match = re.search(r\"Action Input:\\s*(.*)\", model_response, re.DOTALL)\n",
        "\n",
        "\n",
        "if code_match:\n",
        "    generated_code = code_match.group(1).strip()\n",
        "    print(\"\\n--- Extracted Code to be Tested ---\")\n",
        "    print(generated_code)\n",
        "\n",
        "    # Combine the model's generated function with the official unit test\n",
        "    full_test_code = generated_code + \"\\n\\n\" + humaneval_test\n",
        "\n",
        "    print(\"\\n--- Executing Full Test Code in REPL ---\")\n",
        "    print(full_test_code)\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # Run the combined code using our REPL tool\n",
        "    # The unit test uses `assert`, which will raise an error if the test fails.\n",
        "    try:\n",
        "        result = repl_tool.run(full_test_code)\n",
        "        # If the code runs without error and the result is empty, the assert passed.\n",
        "        # Sometimes the REPL might return an object representation, but an AssertionError is the key failure signal.\n",
        "        print(f\"\\nREPL Output: {result}\")\n",
        "        if \"AssertionError\" in result:\n",
        "             print(\"\\n[VERDICT] ❌ FAIL: The generated code failed the unit test (AssertionError).\")\n",
        "        else:\n",
        "             print(\"\\n[VERDICT] ✅ PASS: The generated code ran successfully and passed the unit test.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any other error occurs during execution.\n",
        "        print(f\"\\nAn error occurred during execution: {e}\")\n",
        "        print(\"\\n[VERDICT] ❌ FAIL: The generated code produced an error during execution.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n[VERDICT] ❌ FAIL: Could not parse the generated code from the model's response. The model did not follow the required format.\")"
      ],
      "metadata": {
        "id": "fr8TXgoWICyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### ERRRORRRR CAUSING CODE - THIS CELL IGNORE ############\n",
        "\n",
        "from datasets import load_dataset\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent\n",
        "from transformers import TextGenerationPipeline\n",
        "import re\n",
        "python_repl_tool = PythonREPLTool()\n",
        "\n",
        "hf_pipeline = TextGenerationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",  # explicitly set task\n",
        "    temperature=0.2,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "agent = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=python_repl_tool,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "humaneval_dataset = load_dataset(\"openai_humaneval\")\n",
        "problem_index = 0  # choose any index\n",
        "problem = humaneval_dataset[\"test\"][problem_index]\n",
        "\n",
        "humaneval_prompt = problem[\"prompt\"]\n",
        "humaneval_test = problem[\"test\"]\n",
        "entry_point = problem[\"entry_point\"]\n",
        "\n",
        "print(f\"Loaded HumanEval Problem #{problem_index}: {entry_point}\")\n",
        "print(\"\\n--- Function Stub ---\")\n",
        "print(humaneval_prompt)\n",
        "print(\"\\n--- Unit Test ---\")\n",
        "print(humaneval_test)\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are an expert Python programmer.\n",
        "Your task is to complete the following function.\n",
        "ONLY output the Python code. Do NOT include explanations, thoughts, or 'Final Answer' text.\n",
        "Do not include markdown or ```python fences.\n",
        "Here is the function to complete:\n",
        "\n",
        "{humaneval_prompt}\n",
        "\"\"\"\n",
        "\n",
        "model_response = agent.run(input=prompt_template, handle_parsing_errors=True)\n",
        "\n",
        "print(\"\\n--- Model Generated Code ---\")\n",
        "print(model_response)\n",
        "\n",
        "lines = model_response.splitlines()\n",
        "generated_code = \"\\n\".join(line for line in lines if not line.strip().startswith(\"Final Answer\"))\n",
        "\n",
        "print(\"\\n--- Cleaned Generated Code ---\")\n",
        "print(generated_code)\n",
        "# ==============================================================================\n",
        "full_test_code = generated_code + \"\\n\\n\" + humaneval_test\n",
        "\n",
        "print(\"\\n--- Executing Combined Code (Generated + Unit Test) ---\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "# Run in REPL and check results\n",
        "try:\n",
        "    result = python_repl_tool.run(full_test_code)\n",
        "    print(f\"\\nREPL Output:\\n{result}\")\n",
        "    if \"AssertionError\" in result:\n",
        "        print(\"\\n[VERDICT] ❌ FAIL: The generated code failed the unit test (AssertionError).\")\n",
        "    else:\n",
        "        print(\"\\n[VERDICT] ✅ PASS: The generated code passed the unit test.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during execution: {e}\")\n",
        "    print(\"\\n[VERDICT] ❌ FAIL: The generated code produced an error.\")\n"
      ],
      "metadata": {
        "id": "JRV6xrfCMGBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import TextGenerationPipeline\n",
        "import re\n",
        "\n",
        "\n",
        "python_repl_tool = PythonREPLTool()\n",
        "\n",
        "hf_pipeline = TextGenerationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "humaneval_dataset = load_dataset(\"openai_humaneval\")\n",
        "problem_index = 0\n",
        "problem = humaneval_dataset[\"test\"][problem_index]\n",
        "\n",
        "prompt = problem[\"prompt\"]\n",
        "unit_test = problem[\"test\"]\n",
        "\n",
        "print(f\"Loaded HumanEval problem #{problem_index}: {problem['entry_point']}\\n\")\n",
        "print(\"--- Function Stub ---\")\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "clean_prompt = f\"\"\"\n",
        "You are an expert Python programmer.\n",
        "ONLY output valid Python code for the following function.\n",
        "Do NOT include explanations, thoughts, markdown, or Final Answer text.\n",
        "\n",
        "Function to complete:\n",
        "\n",
        "{prompt}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model_response = llm(clean_prompt)\n",
        "generated_code = model_response.strip()\n",
        "\n",
        "print(\"\\n--- Generated Code ---\")\n",
        "print(generated_code)\n",
        "\n",
        "full_code = generated_code + \"\\n\\n\" + unit_test\n",
        "\n",
        "try:\n",
        "    result = python_repl_tool.run(full_code)\n",
        "    print(\"\\n--- REPL Output ---\")\n",
        "    print(result)\n",
        "    if \"AssertionError\" in result:\n",
        "        print(\"\\n[VERDICT] ❌ FAIL\")\n",
        "    else:\n",
        "        print(\"\\n[VERDICT] ✅ PASS\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during execution: {e}\")\n",
        "    print(\"\\n[VERDICT] ❌ FAIL\")\n"
      ],
      "metadata": {
        "id": "wk5TD7TTOE4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import TextGenerationPipeline\n",
        "\n",
        "python_repl_tool = PythonREPLTool()\n",
        "hf_pipeline = TextGenerationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "humaneval_dataset = load_dataset(\"openai_humaneval\")\n",
        "test_set = humaneval_dataset[\"test\"]\n",
        "\n",
        "\n",
        "N = 5\n",
        "if N is not None:\n",
        "    test_set = test_set.select(range(N))\n",
        "\n",
        "\n",
        "test_set_list = [dict(row) for row in test_set]\n",
        "\n",
        "for idx, problem in enumerate(test_set_list):\n",
        "    prompt = problem[\"prompt\"]\n",
        "    unit_test = problem[\"test\"]\n",
        "    entry_point = problem[\"entry_point\"]\n",
        "    print(f\"\\n=== Problem #{idx}: {entry_point} ===\\n\")\n",
        "\n",
        "    clean_prompt = f\"\"\"\n",
        "You are an expert Python programmer.\n",
        "ONLY output valid Python code for the following function.\n",
        "Do NOT include explanations, thoughts, markdown, or Final Answer text.\n",
        "\n",
        "Function to complete:\n",
        "\n",
        "{prompt}\n",
        "\"\"\"\n",
        "\n",
        "    # Generate code\n",
        "    try:\n",
        "        model_response = llm(clean_prompt)  # returns string\n",
        "        generated_code = model_response.strip()  # treat as string\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {e}\")\n",
        "        results.append((entry_point, False, \"Generation error\"))\n",
        "        continue\n",
        "\n",
        "    print(\"\\n--- Generated Code ---\")\n",
        "    print(generated_code)\n",
        "\n",
        "    # Combine generated code with unit test\n",
        "    full_code = generated_code + \"\\n\\n\" + unit_test\n",
        "\n",
        "    # Run in Python REPL\n",
        "    try:\n",
        "        output = python_repl_tool.run(full_code)\n",
        "        passed = \"AssertionError\" not in output\n",
        "        results.append((entry_point, passed, output))\n",
        "        status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
        "        print(f\"\\n{status}\\n\")\n",
        "    except Exception as e:\n",
        "        results.append((entry_point, False, f\"Execution error: {e}\"))\n",
        "        print(f\"\\n❌ FAIL: Execution error: {e}\\n\")\n",
        "\n",
        "print(\"\\n=== HumanEval Summary ===\\n\")\n",
        "for entry_point, passed, _ in results:\n",
        "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
        "    print(f\"{entry_point}: {status}\")\n",
        "\n",
        "# Total passed\n",
        "total_passed = sum(1 for _, passed, _ in results if passed)\n",
        "print(f\"\\nTotal Passed: {total_passed} / {len(results)}\")\n"
      ],
      "metadata": {
        "id": "_OuOYizyOZ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#error...\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 1: Install All Necessary Libraries\n",
        "# ==============================================================================\n",
        "# !pip install -qU torch transformers bitsandbytes accelerate\n",
        "# !pip install -qU langchain langchain-experimental langgraph datasets langchain-huggingface\n",
        "\n",
        "# print(\"✅ All libraries installed.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2: All Necessary Imports\n",
        "# ==============================================================================\n",
        "import operator\n",
        "import re\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "\n",
        "# For model loading\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# For the agent components\n",
        "from langchain.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# For the agent graph and evaluation\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"✅ All modules imported.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3: Load Model, Tokenizer, and Create the REPL Tool\n",
        "# ==============================================================================\n",
        "# print(\"\\n--- Loading base model and tokenizer ---\")\n",
        "# model_id = \"bigcode/starcoderbase-3b\"\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
        "# )\n",
        "# print(\"✅ Model and tokenizer are ready.\")\n",
        "\n",
        "# # --- Create the REPL Tool ---\n",
        "# python_repl_utility = PythonREPL()\n",
        "# repl_tool = Tool(\n",
        "#     name=\"python_repl\",\n",
        "#     description=\"A Python REPL. Use this to execute python code. The input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "#     func=python_repl_utility.run\n",
        "# )\n",
        "# print(\"✅ Python REPL tool is ready.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4: Build the Complete A.C.E. Agent (The Reusable Problem Solver)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Building the autonomous A.C.E. agent ---\")\n",
        "\n",
        "# --- Create LangChain Pipeline and bind the tool ---\n",
        "\n",
        "\n",
        "hf_pipeline = TextGenerationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",  # explicitly set\n",
        "    temperature=0.2,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# --- Define Prompt, State, Nodes, and Logic ---\n",
        "agent_prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are A.C.E., an expert Python programming agent. Your goal is to solve the user's task.\n",
        "You MUST call the python_repl tool with the complete, runnable Python code to solve the problem.\n",
        "The code you provide should include the function definition and the necessary checks or assertions.\n",
        "\n",
        "User's Task: {input}\n",
        "\"\"\")\n",
        "agent_chain = agent_prompt | llm_with_tools\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "def call_model(state):\n",
        "    response = agent_chain.invoke({\"input\": state['messages'][-1].content})\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "tool_node = ToolNode([repl_tool])\n",
        "\n",
        "def should_continue(state):\n",
        "    if hasattr(state['messages'][-1], \"tool_calls\") and state['messages'][-1].tool_calls:\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"end\"\n",
        "\n",
        "# --- Assemble and Compile the Agent Graph ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", tool_node)\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"action\", \"end\": END})\n",
        "workflow.add_edge('action', 'agent')\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"✅ A.C.E. agent has been built and compiled.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 5: Run the HumanEval Loop Using the A.C.E. Agent\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting HumanEval evaluation with the A.C.E. Agent ---\")\n",
        "\n",
        "humaneval_dataset = load_dataset(\"openai_humaneval\")\n",
        "test_set = humaneval_dataset[\"test\"]\n",
        "\n",
        "# --- Configuration ---\n",
        "results = []\n",
        "N = 5  # Set how many problems to evaluate\n",
        "if N is not None:\n",
        "    test_set = test_set.select(range(N))\n",
        "\n",
        "# --- The Evaluation Loop ---\n",
        "for idx, problem in enumerate(test_set):\n",
        "    prompt = problem[\"prompt\"]\n",
        "    unit_test = problem[\"test\"]\n",
        "    entry_point = problem[\"entry_point\"]\n",
        "    print(f\"\\n=== Problem #{idx}: {entry_point} ===\\n\")\n",
        "\n",
        "    # Define the full task for the agent\n",
        "    # We tell the agent to complete the function AND ensure it passes the provided test\n",
        "    full_task = f\"\"\"\n",
        "Complete the following Python function:\n",
        "{prompt}\n",
        "\n",
        "Your solution must pass this unit test:\n",
        "{unit_test}\n",
        "\n",
        "Please provide the complete, runnable code including the function and the test in the python_repl tool.\n",
        "\"\"\"\n",
        "    # Create the initial input for the agent\n",
        "    inputs = {\"messages\": [HumanMessage(content=full_task)]}\n",
        "\n",
        "    # --- Invoke the AGENT, not the LLM directly ---\n",
        "    try:\n",
        "        final_state = app.invoke(inputs)\n",
        "        # The agent's final message contains the result from the REPL\n",
        "        final_tool_output = final_state['messages'][-1].content\n",
        "\n",
        "        # Check for assertion errors in the tool's output\n",
        "        passed = \"AssertionError\" not in final_tool_output\n",
        "        results.append((entry_point, passed, final_tool_output))\n",
        "        status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
        "        print(f\"\\n--- Agent's Final Output ---\\n{final_tool_output}\")\n",
        "        print(f\"\\n{status}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results.append((entry_point, False, f\"Agent invocation error: {e}\"))\n",
        "        print(f\"\\n❌ FAIL: Agent invocation error: {e}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 6: Summary Report\n",
        "# ==============================================================================\n",
        "print(\"\\n=== HumanEval Summary (Agent Performance) ===\\n\")\n",
        "for entry_point, passed, _ in results:\n",
        "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
        "    print(f\"{entry_point}: {status}\")\n",
        "\n",
        "# Calculate Pass@1\n",
        "total_passed = sum(1 for _, passed, _ in results if passed)\n",
        "pass_rate = (total_passed / len(results)) * 100\n",
        "print(f\"\\nTotal Passed: {total_passed} / {len(results)}\")\n",
        "print(f\"Pass@1 Rate: {pass_rate:.2f}%\")"
      ],
      "metadata": {
        "id": "euxd0HXVPrZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}